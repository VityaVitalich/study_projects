{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Advantage-Actor Critic (A2C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from atari_wrappers import nature_dqn_env\n",
    "\n",
    "nenvs = 8  # change this if you have more than 8 CPU ;)\n",
    "\n",
    "env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=nenvs)\n",
    "\n",
    "n_actions = env.action_space.spaces[0].n\n",
    "obs = env.reset()\n",
    "assert obs.shape == (nenvs, 4, 84, 84)\n",
    "assert obs.dtype == np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "\n",
    "\n",
    "def weights_init_orthogonal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # print(classname)\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        init.orthogonal_(m.weight.data, gain=np.sqrt(2))\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        init.orthogonal_(m.weight.data, gain=np.sqrt(2))\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class ACAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        # conv2d_size_out(conv2d_size_out(conv2d_size_out(64, 3, 2), 3, 2), 3, 2)\n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten(),\n",
    "            nn.Linear(5184, 256),\n",
    "        )\n",
    "\n",
    "        self.action_head = nn.Sequential(\n",
    "            nn.Linear(256, 256), nn.ReLU(), nn.Linear(256, n_actions)\n",
    "        )\n",
    "        self.V_head = nn.Sequential(nn.Linear(256, 256), nn.ReLU(), nn.Linear(256, 1))\n",
    "\n",
    "        self.net.apply(weights_init_orthogonal)\n",
    "        self.action_head.apply(weights_init_orthogonal)\n",
    "        self.V_head.apply(weights_init_orthogonal)\n",
    "\n",
    "    def forward(self, states):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            states - tensor, (batch_size x channels x width x height)\n",
    "        output:\n",
    "            logits - tensor, logits of action probabilities for your actor policy, (batch_size x num_actions)\n",
    "            V - tensor, critic estimation, (batch_size)\n",
    "        \"\"\"\n",
    "\n",
    "        features = self.net(states)\n",
    "        logits = self.action_head(features)\n",
    "        V = self.V_head(features)\n",
    "\n",
    "        return logits, V.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def act(self, inputs):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            inputs - numpy array, (batch_size x channels x width x height)\n",
    "        output: dict containing keys ['actions', 'logits', 'log_probs', 'values']:\n",
    "            'actions' - selected actions, numpy, (batch_size)\n",
    "            'logits' - actions logits, tensor, (batch_size x num_actions)\n",
    "            'log_probs' - log probs of selected actions, tensor, (batch_size)\n",
    "            'values' - critic estimations, tensor, (batch_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # print(inputs.shape)\n",
    "        inputs = torch.tensor(inputs).to(self.device)\n",
    "        logits, V = self.model(inputs)\n",
    "        dist = Categorical(logits=logits)\n",
    "        actions = dist.sample().cpu().numpy()\n",
    "\n",
    "        log_proba = torch.log(nn.functional.softmax(logits, dim=-1))\n",
    "        log_probs = log_proba[range(log_proba.size()[0]), actions]\n",
    "\n",
    "        entropy = (\n",
    "            nn.functional.softmax(logits, dim=-1)\n",
    "            * nn.functional.log_softmax(logits, dim=-1)\n",
    "        ).sum(1)\n",
    "\n",
    "        return {\n",
    "            \"actions\": actions,\n",
    "            \"logits\": logits,\n",
    "            \"log_probs\": log_probs,\n",
    "            \"values\": V,\n",
    "            \"entropy\": entropy,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from runners import EnvRunner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This runner interacts with the environment for a given number of steps and returns a dictionary containing\n",
    "keys \n",
    "\n",
    "* 'observations' \n",
    "* 'rewards' \n",
    "* 'dones'\n",
    "* 'actions'\n",
    "* all other keys that you defined in `Policy`\n",
    "\n",
    "under each of these keys there is a python `list` of interactions with the environment of specified length $T$ &mdash; the size of partial trajectory, or rollout length. Let's have a look at how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ACAgent((4, 84, 84), n_actions)\n",
    "policy = Policy(model, \"cuda\")\n",
    "runner = EnvRunner(env, policy, nsteps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates new rollout\n",
    "trajectory = runner.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['actions', 'logits', 'log_probs', 'values', 'entropy', 'observations', 'rewards', 'dones'])\n"
     ]
    }
   ],
   "source": [
    "# what is inside\n",
    "print(trajectory.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "assert \"logits\" in trajectory, \"Not found: policy didn't provide logits\"\n",
    "assert (\n",
    "    \"log_probs\" in trajectory\n",
    "), \"Not found: policy didn't provide log_probs of selected actions\"\n",
    "assert \"values\" in trajectory, \"Not found: policy didn't provide critic estimations\"\n",
    "assert trajectory[\"logits\"][0].shape == (nenvs, n_actions), \"logits wrong shape\"\n",
    "assert trajectory[\"log_probs\"][0].shape == (nenvs,), \"log_probs wrong shape\"\n",
    "assert trajectory[\"values\"][0].shape == (nenvs,), \"values wrong shape\"\n",
    "\n",
    "for key in trajectory.keys():\n",
    "    assert (\n",
    "        len(trajectory[key]) == 5\n",
    "    ), f\"something went wrong: 5 steps should have been done, got trajectory of length {len(trajectory[key])} for '{key}'\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The formula for the value targets is simple:\n",
    "\n",
    "$$\n",
    "\\hat v(s_t) = \\sum_{t'=0}^{T - 1}\\gamma^{t'}r_{t+t'} + \\gamma^T \\hat{v}(s_{t+T}),\n",
    "$$\n",
    "\n",
    "where $s_{t + T}$ is the latest observation of the environment.\n",
    "\n",
    "Any callable could be passed to `EnvRunner` to be applied to each partial trajectory after it is collected. \n",
    "Thus, we can implement and use `ComputeValueTargets` callable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeValueTargets:\n",
    "    def __init__(self, policy, gamma=0.99):\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __call__(self, trajectory, latest_observation):\n",
    "        \"\"\"\n",
    "        This method should modify trajectory inplace by adding\n",
    "        an item with key 'value_targets' to it\n",
    "\n",
    "        input:\n",
    "            trajectory - dict from runner\n",
    "            latest_observation - last state, numpy, (num_envs x channels x width x height)\n",
    "        \"\"\"\n",
    "        num_steps, num_envs = np.vstack(trajectory[\"rewards\"]).shape\n",
    "\n",
    "        out_policy = self.policy.act(latest_observation)\n",
    "        values = out_policy[\"values\"]\n",
    "\n",
    "        value_targets = torch.cat(\n",
    "            [\n",
    "                torch.zeros((num_steps, num_envs), device=self.policy.device),\n",
    "                values.unsqueeze(0),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for t in reversed(range(num_steps)):\n",
    "            value_targets[t] = torch.tensor(trajectory[\"rewards\"][t]).to(\n",
    "                self.policy.device\n",
    "            ) + self.gamma * value_targets[t + 1] * torch.tensor(\n",
    "                (1 - trajectory[\"dones\"][t])\n",
    "            ).to(\n",
    "                self.policy.device\n",
    "            )\n",
    "\n",
    "        # rewards = np.vstack(trajectory['rewards']) * (1 - np.vstack(trajectory['dones']))\n",
    "\n",
    "        # discounted = rewards *  np.power(np.repeat(self.gamma, num_steps), range(num_steps))[:,np.newaxis]\n",
    "        # value_targets = np.flip(np.cumsum(np.flip(discounted), axis=0))\n",
    "\n",
    "        # value_targets = torch.tensor(value_targets.copy(), dtype=torch.float32).to(policy.device) + values * self.gamma ** (num_steps + 1)\n",
    "\n",
    "        trajectory[\"value_targets\"] = value_targets[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeTimeBatch:\n",
    "    \"\"\"Merges first two axes typically representing time and env batch.\"\"\"\n",
    "\n",
    "    def __call__(self, trajectory, latest_observation):\n",
    "        # Modify trajectory inplace.\n",
    "        num_steps, num_envs = np.vstack(trajectory[\"rewards\"]).shape\n",
    "\n",
    "        trajectory[\"value_targets\"] = (\n",
    "            trajectory[\"value_targets\"].view(num_steps * num_envs, -1).squeeze(1)\n",
    "        )\n",
    "        trajectory[\"values\"] = (\n",
    "            torch.stack(trajectory[\"values\"]).view(num_steps * num_envs, -1).squeeze(1)\n",
    "        )\n",
    "        trajectory[\"log_probs\"] = (\n",
    "            torch.stack(trajectory[\"log_probs\"])\n",
    "            .view(num_steps * num_envs, -1)\n",
    "            .squeeze(1)\n",
    "        )\n",
    "        trajectory[\"entropy\"] = (\n",
    "            torch.stack(trajectory[\"entropy\"]).view(num_steps * num_envs, -1).squeeze(1)\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do more sanity checks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = EnvRunner(\n",
    "    env, policy, nsteps=5, transforms=[ComputeValueTargets(policy), MergeTimeBatch()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = runner.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More sanity checks\n",
    "assert \"value_targets\" in trajectory, \"Value targets not found\"\n",
    "assert trajectory[\"log_probs\"].shape == (5 * nenvs,)\n",
    "assert trajectory[\"value_targets\"].shape == (5 * nenvs,)\n",
    "assert trajectory[\"values\"].shape == (5 * nenvs,)\n",
    "\n",
    "assert trajectory[\n",
    "    \"log_probs\"\n",
    "].requires_grad, \"Gradients are not available for actor head!\"\n",
    "assert trajectory[\n",
    "    \"values\"\n",
    "].requires_grad, \"Gradients are not available for critic head!\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the time to implement the advantage actor critic algorithm itself. You can look into [Mnih et al. 2016](https://arxiv.org/abs/1602.01783) paper, and lectures ([part 1](https://www.youtube.com/watch?v=Ds1trXd6pos&list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&index=5), [part 2](https://www.youtube.com/watch?v=EKqxumCuAAY&list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&index=6)) by Sergey Levine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "\n",
    "class A2C:\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy,\n",
    "        optimizer,\n",
    "        value_loss_coef=0.25,\n",
    "        entropy_coef=0.01,\n",
    "        max_grad_norm=0.5,\n",
    "    ):\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def loss(self, trajectory, write):\n",
    "        # compute all losses\n",
    "        # do not forget to use weights for critic loss and entropy loss\n",
    "\n",
    "        advantage = trajectory[\"value_targets\"].detach() - trajectory[\"values\"]\n",
    "        policy_loss = torch.mean(-trajectory[\"log_probs\"] * advantage.detach())\n",
    "\n",
    "        critic_loss = advantage.abs().mean()\n",
    "\n",
    "        entropy_loss = trajectory[\"entropy\"].mean()\n",
    "\n",
    "        # log all losses\n",
    "        write(\n",
    "            \"losses\",\n",
    "            {\n",
    "                \"policy loss\": policy_loss,\n",
    "                \"critic loss\": critic_loss,\n",
    "                \"entropy loss\": entropy_loss,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # additional logs\n",
    "        write(\"critic/advantage\", advantage.mean())\n",
    "        write(\n",
    "            \"critic/values\",\n",
    "            {\n",
    "                \"value predictions\": trajectory[\"values\"].mean(),\n",
    "                \"value targets\": trajectory[\"value_targets\"].mean(),\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # return scalar loss\n",
    "        return (\n",
    "            self.value_loss_coef * critic_loss\n",
    "            + policy_loss\n",
    "            + self.entropy_coef * entropy_loss\n",
    "        )\n",
    "\n",
    "    def train(self, runner):\n",
    "        # collect trajectory using runner\n",
    "        # compute loss and perform one step of gradient optimization\n",
    "        # do not forget to clip gradients\n",
    "        trajectory = runner.get_next()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss(trajectory, runner.write)\n",
    "        loss.backward()\n",
    "\n",
    "        clip_grad_norm_(self.policy.model.parameters(), self.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        total_norm = 0\n",
    "        for p in self.policy.model.parameters():\n",
    "            param_norm = p.grad.detach().data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm**0.5\n",
    "\n",
    "        # use runner.write to log scalar to tensorboard\n",
    "        runner.write(\"gradient norm\", total_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ACAgent((4, 84, 84), 6)\n",
    "policy = Policy(model, \"cuda\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "a2c = A2C(\n",
    "    policy,\n",
    "    optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = EnvRunner(\n",
    "    env, policy, nsteps=20, transforms=[ComputeValueTargets(policy), MergeTimeBatch()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [1:22:51<00:00,  5.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 25000\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    a2c.train(runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your model just in case\n",
    "torch.save(model.state_dict(), \"A2C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = nature_dqn_env(\n",
    "    \"SpaceInvadersNoFrameskip-v4\",\n",
    "    nenvs=None,\n",
    "    clip_reward=False,\n",
    "    summaries=False,\n",
    "    episodic_life=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, policy, n_games=1, t_max=10000):\n",
    "    \"\"\"\n",
    "    Plays n_games and returns rewards\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "\n",
    "        R = 0\n",
    "        for _ in range(t_max):\n",
    "            action = policy.act(np.array([s]))[\"actions\"][0]\n",
    "\n",
    "            s, r, done, _ = env.step(action)\n",
    "\n",
    "            R += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(R)\n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your score: 577.3333333333334\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "# evaluation will take some time!\n",
    "sessions = evaluate(env, policy, n_games=30)\n",
    "score = sessions.mean()\n",
    "print(f\"Your score: {score}\")\n",
    "\n",
    "assert score >= 500, \"Needs more training?\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_monitor = nature_dqn_env(\n",
    "    \"SpaceInvadersNoFrameskip-v4\",\n",
    "    nenvs=None,\n",
    "    monitor=True,\n",
    "    clip_reward=False,\n",
    "    summaries=False,\n",
    "    episodic_life=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# record sessions\n",
    "sessions = evaluate(env_monitor, policy, n_games=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([510., 500., 800.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rewards for recorded games\n",
    "sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
