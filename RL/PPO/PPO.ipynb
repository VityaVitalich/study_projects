{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'lib/pybullet-gym'...\n",
      "remote: Enumerating objects: 804, done.\u001b[K\n",
      "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
      "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
      "remote: Total 804 (delta 21), reused 44 (delta 17), pack-reused 750\u001b[K\n",
      "Receiving objects: 100% (804/804), 19.31 MiB | 20.08 MiB/s, done.\n",
      "Resolving deltas: 100% (437/437), done.\n",
      "Obtaining file:///data/home/vitya/DataPrep/experiments/od/PPO/lib/pybullet-gym\n",
      "Collecting pybullet>=1.7.8\n",
      "  Downloading pybullet-3.2.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (91.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 91.7 MB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pybullet, pybulletgym\n",
      "  Running setup.py develop for pybulletgym\n",
      "Successfully installed pybullet-3.2.5 pybulletgym\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/benelot/pybullet-gym lib/pybullet-gym\n",
    "!pip install -e lib/pybullet-gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pybulletgym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WalkerBase::__init__\n",
      "argv[0]=\n",
      "observation space:  Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32) \n",
      "observations: [ 0.00092668  0.00026526 -0.00041955 -0.00079245  0.00034591 -0.00021478\n",
      "  0.00033946 -0.00087684  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.        ]\n",
      "action space:  Box([-1. -1. -1. -1. -1. -1.], [1. 1. 1. 1. 1. 1.], (6,), float32) \n",
      "action_sample:  [ 0.4164298  -0.9497912  -0.6045943   0.6782092   0.27260754 -0.99954736]\n",
      "argv[0]=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/vitya/DataPrep/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "from mujoco_wrappers import Normalize\n",
    "from logger import TensorboardSummaries as Summaries\n",
    "\n",
    "env = gym.make(\"HalfCheetahMuJoCoEnv-v0\")\n",
    "env = Normalize(Summaries(env))\n",
    "env.unwrapped.seed(0)\n",
    "\n",
    "print(\"observation space: \", env.observation_space, \"\\nobservations:\", env.reset())\n",
    "print(\n",
    "    \"action space: \", env.action_space, \"\\naction_sample: \", env.action_space.sample()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_orthogonal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # print(classname)\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        init.orthogonal_(m.weight.data, gain=np.sqrt(2))\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        init.orthogonal_(m.weight.data, gain=np.sqrt(2))\n",
    "        init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "class PPOModel(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.obs_shape = obs_shape\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # conv2d_size_out(conv2d_size_out(conv2d_size_out(64, 3, 2), 3, 2), 3, 2)\n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(self.obs_shape, self.hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.mean_head = nn.Linear(self.hidden_dim, self.n_actions)\n",
    "        self.covariance_head = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.n_actions), nn.Softplus()\n",
    "        )\n",
    "        self.value_head = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "        # self.backbone.apply(weights_init_orthogonal)\n",
    "        # self.mean_head.apply(weights_init_orthogonal)\n",
    "        # self.covariance_head.apply(weights_init_orthogonal)\n",
    "        # self.value_head.apply(weights_init_orthogonal)\n",
    "\n",
    "    def forward(self, states):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            states - tensor, (batch_size x features)\n",
    "        output:\n",
    "            mean - tensor, (batch_size x actions_dim)\n",
    "            cov - tensor, (batch_size x actions_dim)\n",
    "            V - tensor, critic estimation, (batch_size)\n",
    "        \"\"\"\n",
    "\n",
    "        features = self.backbone(states)\n",
    "        mean = self.mean_head(features)\n",
    "        cov = self.covariance_head(features)\n",
    "        V = self.value_head(features)\n",
    "\n",
    "        # print(V.size())\n",
    "        return mean, cov, V.squeeze()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model will be wrapped by a `Policy`. The policy can work in two modes, but in either case \n",
    "it is going to return dictionary with string-type keys. The first mode is when the policy is \n",
    "used to sample actions for a trajectory which will later be used for training. In this case \n",
    "the flag `training` passed to `act` method is `False` and the method should return \n",
    "a `dict` with the following keys: \n",
    "\n",
    "* `\"actions\"`: actions to pass to the environment\n",
    "* `\"log_probs\"`: log-probabilities of sampled actions\n",
    "* `\"values\"`: value function $V^\\pi(s)$ predictions.\n",
    "\n",
    "We don't need to use the values under these keys for training, so all of them should be of type `np.ndarray`. This regime will be used to collect data.\n",
    "\n",
    "When `training` is `True`, the model is training on a given batch of observations. In this\n",
    "case it should return a `dict` with the following keys\n",
    "\n",
    "* `\"distribution\"`: an instance of multivariate normal distribution (`torch.distributions.MultivariateNormal`)\n",
    "* `\"values\"`: value function $V^\\pi(s)$ prediction, tensor\n",
    "\n",
    "The distinction about the modes comes into play depending on where the policy is used: if it is called from `EnvRunner`, \n",
    "the `training` flag is `False`, if it is called from `PPO`, the `training` flag is `True`. These classes \n",
    "will be described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.n_actions = self.model.n_actions\n",
    "\n",
    "    def act(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            inputs - numpy array if training is False, otherwise tensor, (batch_size x features)\n",
    "            training - flag, bool\n",
    "        output:\n",
    "            if training is True, dict containing keys ['actions', 'log_probs', 'values']:\n",
    "                `distribution` - MultivariateNormal, (batch_size x actions_dim)\n",
    "                'values' - critic estimations, tensor, (batch_size)\n",
    "            if training is False, dict containing keys ['actions', 'log_probs', 'values']:\n",
    "                'actions' - selected actions, numpy, (batch_size)\n",
    "                'log_probs' - log probs of selected actions, numpy, (batch_size)\n",
    "                'values' - critic estimations, numpy, (batch_size)\n",
    "        \"\"\"\n",
    "        # if training is false, input is numpy\n",
    "\n",
    "        if not training:\n",
    "            inputs = torch.FloatTensor(inputs).to(DEVICE)\n",
    "\n",
    "        mean, cov, V = self.model(inputs)\n",
    "        m = MultivariateNormal(mean, torch.diag_embed(cov))\n",
    "\n",
    "        actions = m.sample()\n",
    "        log_probs = m.log_prob(actions)\n",
    "\n",
    "        if training:\n",
    "            return {\"distribution\": m, \"values\": V}\n",
    "        else:\n",
    "            return {\n",
    "                \"actions\": actions.detach().cpu().numpy(),\n",
    "                \"log_probs\": log_probs.detach().cpu().numpy(),\n",
    "                \"values\": V.detach().cpu().numpy(),\n",
    "            }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `EnvRunner` to perform interactions with an environment with a policy for a fixed number of timesteps. Calling `.get_next()` on a runner will return a trajectory &mdash; dictionary \n",
    "containing keys\n",
    "\n",
    "* `\"observations\"`\n",
    "* `\"rewards\"` \n",
    "* `\"dones\"`\n",
    "* `\"actions\"`\n",
    "* all other keys that you defined in `Policy` in `training=False` regime,\n",
    "\n",
    "under each of these keys there is a `np.ndarray` of specified length $T$ &mdash; the size of partial trajectory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from runners import EnvRunner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, before returning a trajectory this runner can apply a list of transformations. \n",
    "Each transformation is simply a callable that should modify passed trajectory in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class AsArray:\n",
    "    \"\"\"\n",
    "    Converts lists of interactions to ndarray.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, trajectory, last_observation):\n",
    "        # Modifies trajectory inplace.\n",
    "        # Just switches python lists to numpy arrays\n",
    "        for k, v in trajectory.items():\n",
    "            trajectory[k] = np.asarray(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPOModel(17, 6, 16).to(DEVICE)\n",
    "policy = Policy(model)\n",
    "runner = EnvRunner(env, policy, nsteps=5, transforms=[AsArray()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates new rollout\n",
    "trajectory = runner.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['actions', 'log_probs', 'values', 'observations', 'rewards', 'dones'])\n"
     ]
    }
   ],
   "source": [
    "# what is inside\n",
    "print(trajectory.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.9360738 ,  0.949878  , -0.6336447 , -0.09170941,  0.8897381 ,\n",
       "         0.29848593],\n",
       "       [ 0.3197962 ,  0.01018037, -0.5062505 , -0.0810163 , -0.28351927,\n",
       "        -0.13856964],\n",
       "       [ 0.6331702 , -0.39455462,  0.6109785 , -0.06013709, -0.63963383,\n",
       "         0.49694097],\n",
       "       [-0.40845338, -0.27975643, -0.20311242,  0.49331617, -0.11180159,\n",
       "         1.0350164 ],\n",
       "       [-1.0078186 , -0.76484746,  0.08241332, -0.3181476 , -0.97477615,\n",
       "        -0.504859  ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory[\"actions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "assert (\n",
    "    \"log_probs\" in trajectory\n",
    "), \"Not found: policy didn't provide log_probs of selected actions\"\n",
    "assert \"values\" in trajectory, \"Not found: policy didn't provide critic estimations\"\n",
    "assert trajectory[\"log_probs\"].shape == (5,), \"log_probs wrong shape\"\n",
    "assert trajectory[\"values\"].shape == (5,), \"values wrong shape\"\n",
    "assert trajectory[\"observations\"].shape == (5, 17), \"observations wrong shape\"\n",
    "assert trajectory[\"rewards\"].shape == (5,), \"rewards wrong shape\"\n",
    "assert trajectory[\"dones\"].shape == (5,), \"dones wrong shape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actions': (5, 6),\n",
       " 'log_probs': (5,),\n",
       " 'values': (5,),\n",
       " 'observations': (5, 17),\n",
       " 'rewards': (5,),\n",
       " 'dones': (5,)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is what collected inside\n",
    "{k: v.shape for k, v in trajectory.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first is `GAE` that implements [Generalized Advantage Estimator](https://arxiv.org/abs/1506.02438).\n",
    "In it you should add two keys to the trajectory: `\"advantages\"` and `\"value_targets\"`. In GAE the advantages\n",
    "$A_t^{\\mathrm{GAE}(\\gamma,\\lambda)}$ are essentially defined as the exponential \n",
    "moving average with parameter $\\lambda$ of the regular advantages \n",
    "$\\hat{A}^{(T)}(s_t) = \\sum_{l=0}^{T-1-t} \\gamma^l r_{t+l} + \\gamma^{T} V^\\pi(s_{T}) - V^\\pi(s_t)$. \n",
    "The exact formula for the computation is the following\n",
    "\n",
    "$$\n",
    "A_{t}^{\\mathrm{GAE}(\\gamma,\\lambda)} = \\sum_{l=0}^{T-1-t} (\\gamma\\lambda)^l\\delta_{t + l}^V, \\, t \\in [0, T)\n",
    "$$\n",
    "where $\\delta_{t+l}^V = r_{t+l} + \\gamma V^\\pi(s_{t+l+1}) - V^\\pi(s_{t+l})$. You can look at the \n",
    "derivation (formulas 11-16) in the paper. Don't forget to reset the summation on terminal\n",
    "states as determined by the flags `trajectory[\"dones\"]`. You can use `trajectory[\"values\"]`\n",
    "to get values of all observations except the most recent which is stored under \n",
    " `trajectory[\"state\"][\"latest_observation\"]`. For this observation you will need to call the policy \n",
    " to get the value prediction.\n",
    "\n",
    "Once you computed the advantages, you can get the targets for training the value function by adding \n",
    "back values:\n",
    "$$\n",
    "\\hat{V}(s_{t+l}) = A_{t+l}^{\\mathrm{GAE}(\\gamma,\\lambda)} + V(s_{t + l}),\n",
    "$$\n",
    "where $\\hat{V}$ is a tensor of value targets that are used to train the value function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAE:\n",
    "    \"\"\"Generalized Advantage Estimator.\"\"\"\n",
    "\n",
    "    def __init__(self, policy, gamma=0.99, lambda_=0.95):\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def __call__(self, trajectory, last_observation):\n",
    "        \"\"\"\n",
    "        This method should modify trajectory inplace by adding\n",
    "        items with keys 'advantages' and 'value_targets' to it\n",
    "\n",
    "        input:\n",
    "            trajectory - dict from runner\n",
    "            latest_observation - last state, numpy, (features)\n",
    "        \"\"\"\n",
    "        num_steps, num_envs = np.vstack(trajectory[\"rewards\"]).shape\n",
    "        out_policy = self.policy.act(last_observation)\n",
    "        values = out_policy[\"values\"]\n",
    "\n",
    "        advantages = np.zeros((num_steps, num_envs), dtype=np.float32)\n",
    "        last_advantage = 0\n",
    "        last_value = values\n",
    "\n",
    "        for t in reversed(range(num_steps)):\n",
    "            mask = 1.0 - trajectory[\"dones\"][t]\n",
    "            last_value = last_value * mask\n",
    "            last_advantage = last_advantage * mask\n",
    "            delta = (\n",
    "                trajectory[\"rewards\"][t]\n",
    "                + self.gamma * last_value\n",
    "                - trajectory[\"values\"][t]\n",
    "            )\n",
    "\n",
    "            last_advantage = delta + self.gamma * self.lambda_ * last_advantage\n",
    "\n",
    "            advantages[t] = last_advantage\n",
    "\n",
    "            last_value = trajectory[\"values\"][t]\n",
    "\n",
    "        if num_envs == 1:\n",
    "            advantages = advantages[:, 0]\n",
    "\n",
    "        trajectory[\"advantages\"] = advantages\n",
    "        trajectory[\"value_targets\"] = trajectory[\"values\"] + trajectory[\"advantages\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a small test just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests\n",
    "class DummyEnv:\n",
    "    def __init__(self):\n",
    "        self.unwrapped = None\n",
    "        self.t = 0\n",
    "        self.state = np.zeros(17)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.state\n",
    "\n",
    "    def step(self, a):\n",
    "        r = [0, -100, 800][self.t]\n",
    "        done = self.t == 2\n",
    "        self.t = (self.t + 1) % 3\n",
    "        return self.state, r, done, {}\n",
    "\n",
    "\n",
    "class DummyPolicy:\n",
    "    def act(self, s):\n",
    "        return {\"values\": np.array(100), \"actions\": np.array([-0.42, 0.42])}\n",
    "\n",
    "\n",
    "dummy_env = DummyEnv()\n",
    "dummy_policy = DummyPolicy()\n",
    "runner = EnvRunner(\n",
    "    dummy_env,\n",
    "    dummy_policy,\n",
    "    nsteps=8,\n",
    "    transforms=[AsArray(), GAE(dummy_policy, gamma=0.8, lambda_=0.5)],\n",
    ")\n",
    "trajectory = runner.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!\n"
     ]
    }
   ],
   "source": [
    "assert \"advantages\" in trajectory, \"Not found: advantage estimation\"\n",
    "assert \"value_targets\" in trajectory, \"Not found: targets for critic\"\n",
    "assert trajectory[\"advantages\"].shape == (8,), \"advantage wrong shape\"\n",
    "assert trajectory[\"value_targets\"].shape == (8,), \"value_targets wrong shape\"\n",
    "assert (\n",
    "    trajectory[\"advantages\"] == np.array([44, 160, 700, 44, 160, 700, -68, -120])\n",
    ").all(), \"advantage computation error\"\n",
    "assert (\n",
    "    trajectory[\"value_targets\"] == trajectory[\"advantages\"] + 100\n",
    ").all(), \"value targets computation error\"\n",
    "print(\"Nice!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    \"\"\"Samples minibatches from trajectory for a number of epochs.\"\"\"\n",
    "\n",
    "    def __init__(self, runner, num_epochs, num_minibatches, transforms=None):\n",
    "        self.runner = runner\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_minibatches = num_minibatches\n",
    "        self.transforms = transforms or []\n",
    "\n",
    "    def get_next(self):\n",
    "        \"\"\"\n",
    "        Yields next minibatch (dict) for training with at least following keys:\n",
    "                'observations' - numpy, (batch_size x features)\n",
    "                'actions' - numpy, (batch_size x actions_dim)\n",
    "                'advantages' - numpy, (batch_size)\n",
    "                'log_probs' - numpy, (batch_size)\n",
    "        \"\"\"\n",
    "        trajectory = self.runner.get_next()\n",
    "        num_steps = len(trajectory[\"values\"])\n",
    "\n",
    "        splited = {}\n",
    "        minibatch = {}\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # shuffle dataset and separate it into minibatches\n",
    "            # you can use any standard utils to do that\n",
    "\n",
    "            idx = np.random.permutation(num_steps)\n",
    "            for key in trajectory.keys():\n",
    "                splited[key] = np.array_split(\n",
    "                    trajectory[key][idx], self.num_minibatches\n",
    "                )\n",
    "\n",
    "            for i in range(self.num_minibatches):\n",
    "                for key in splited.keys():\n",
    "                    minibatch[key] = splited[key][i]\n",
    "\n",
    "                # applying additional transforms\n",
    "                for transform in self.transforms:\n",
    "                    transform(minibatch)\n",
    "\n",
    "                yield minibatch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common trick to use with GAE is to normalize advantages for every minibatch, the following transformation does that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeAdvantages:\n",
    "    \"\"\"Normalizes advantages to have zero mean and variance 1.\"\"\"\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        adv = batch[\"advantages\"]\n",
    "        adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "        batch[\"advantages\"] = adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchify:\n",
    "    \"\"\"Moves everything to PyTorch\"\"\"\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = torch.FloatTensor(v).to(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create our PPO runner! This is our pipeline of data collecting and generating mini-batches for our trainer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ppo_sampler(\n",
    "    env,\n",
    "    policy,\n",
    "    num_runner_steps=2048,\n",
    "    gamma=0.99,\n",
    "    lambda_=0.95,\n",
    "    num_epochs=10,\n",
    "    num_minibatches=32,\n",
    "):\n",
    "    \"\"\"Creates runner for PPO algorithm.\"\"\"\n",
    "    runner_transforms = [AsArray(), GAE(policy, gamma=gamma, lambda_=lambda_)]\n",
    "    runner = EnvRunner(env, policy, num_runner_steps, transforms=runner_transforms)\n",
    "\n",
    "    sampler_transforms = [NormalizeAdvantages(), PyTorchify()]\n",
    "    sampler = Sampler(\n",
    "        runner,\n",
    "        num_epochs=num_epochs,\n",
    "        num_minibatches=num_minibatches,\n",
    "        transforms=sampler_transforms,\n",
    "    )\n",
    "    return sampler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\n",
    "J_{\\pi}(s, a) = \\frac{\\pi_\\theta(a|s)}{\\pi_\\theta^{\\text{old}}(a|s)} \\cdot A^{\\mathrm{GAE}(\\gamma,\\lambda)}(s, a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "J_{\\pi}^{\\text{clipped}}(s, a) = \\mathrm{clip}\\left(\n",
    "\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta^{\\text{old}}}(a|s)},\n",
    "1 - \\text{cliprange}, 1 + \\text{cliprange}\\right)\\cdot A^{\\mathrm{GAE(\\gamma, \\lambda)}}(s)\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{\\text{policy}} = -\\frac{1}{T}\\sum_{l=0}^{T-1}\\min\\left(J_\\pi(s_{t + l}, a_{t + l}), J_{\\pi}^{\\text{clipped}}(s_{t + l}, a_{t + l})\\right).\n",
    "$$\n",
    "\n",
    "The value loss is also modified:\n",
    "\n",
    "$$\n",
    "L_{V}^{\\text{clipped}} = \\frac{1}{T}\\sum_{l=0}^{T-1} \\max(l^{simple}(s_{t + l}), l^{clipped}(s_{t + l})),\n",
    "$$\n",
    "where $l^{simple}$ is your standard critic loss\n",
    "$$\n",
    "l^{simple}(s_{t + l}) = [V_\\theta(s_{t+l}) - \\hat{V}(s_{t + l})]^2\n",
    "$$\n",
    "\n",
    "and $l^{clipped}$ is a clipped version that limits large changes of the value function:\n",
    "$$\n",
    "l^{clipped}(s_{t + l}) = [\n",
    "V_{\\theta^{\\text{old}}}(s_{t+l}) +\n",
    "\\text{clip}\\left(\n",
    "V_\\theta(s_{t+l}) - V_{\\theta^\\text{old}}(s_{t+l}),\n",
    "-\\text{cliprange}, \\text{cliprange}\n",
    "\\right) - \\hat{V}(s_{t + l})] ^ 2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy,\n",
    "        optimizer,\n",
    "        sampler,\n",
    "        cliprange=0.2,\n",
    "        value_loss_coef=0.25,\n",
    "        max_grad_norm=0.5,\n",
    "    ):\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.sampler = sampler\n",
    "        self.cliprange = cliprange\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.iteration = 0\n",
    "\n",
    "    def write(self, name, val):\n",
    "        \"\"\"For logging purposes\"\"\"\n",
    "        self.sampler.runner.write(name, val, self.iteration)\n",
    "\n",
    "    def policy_loss(self, batch, act):\n",
    "        \"\"\"\n",
    "        Computes and returns policy loss on a given minibatch.\n",
    "        input:\n",
    "            batch - dict from sampler, containing:\n",
    "                'advantages' - advantage estimation, tensor, (batch_size)\n",
    "                'actions' - actions selected in real trajectory, (batch_size)\n",
    "                'log_probs' - probabilities of actions from policy used to collect this trajectory, (batch_size)\n",
    "            act - dict from your current policy, containing:\n",
    "                'distribution' - MultivariateNormal, (batch_size x actions_dim)\n",
    "        output:\n",
    "            policy loss - torch scalar\n",
    "        \"\"\"\n",
    "        current_probs = act[\"distribution\"].log_prob(batch[\"actions\"])\n",
    "        r = torch.exp(current_probs - batch[\"log_probs\"])\n",
    "\n",
    "        J = r * batch[\"advantages\"]\n",
    "        J_cliped = (\n",
    "            torch.clip(r, 1 - self.cliprange, 1 + self.cliprange) * batch[\"advantages\"]\n",
    "        )\n",
    "\n",
    "        loss_policy = -torch.mean(torch.min(J, J_cliped))\n",
    "\n",
    "        ratio = (abs(r - 1) > self.cliprange).to(torch.float).mean()\n",
    "        entropy = act[\"distribution\"].entropy()\n",
    "\n",
    "        # additional logs: entropy, fraction of samples for which we zeroed gradient, max ratio\n",
    "        self.write(\"additional/entropy\", entropy.mean())\n",
    "        self.write(\"additional/policy_loss_zeroed_gradient_fraction\", ratio)\n",
    "        self.write(\"additional/max_ratio\", r.max())\n",
    "\n",
    "        return loss_policy\n",
    "\n",
    "    def value_loss(self, batch, act):\n",
    "        \"\"\"\n",
    "        Computes and returns policy loss on a given minibatch.\n",
    "        input:\n",
    "            batch - dict from sampler, containing:\n",
    "                'value_targets' - computed targets for critic, (batch_size)\n",
    "                'values' - critic estimation from network that generated trajectory, (batch_size)\n",
    "            act - dict from your current policy, containing:\n",
    "                'values' - current critic estimation, tensor, (batch_size)\n",
    "        output:\n",
    "            critic loss - torch scalar\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            batch[\"value_targets\"].shape == act[\"values\"].shape\n",
    "        ), \"Danger: your values and value targets have different shape. Watch your broadcasting!\"\n",
    "\n",
    "        critic_loss = (act[\"values\"] - batch[\"value_targets\"].detach()) ** 2\n",
    "        critic_loss_cliped = (\n",
    "            batch[\"values\"]\n",
    "            + torch.clip(\n",
    "                (act[\"values\"] - batch[\"values\"]), -self.cliprange, self.cliprange\n",
    "            )\n",
    "            - batch[\"value_targets\"].detach()\n",
    "        ) ** 2\n",
    "\n",
    "        loss = torch.mean(torch.maximum(critic_loss, critic_loss_cliped))\n",
    "\n",
    "        ratio = (\n",
    "            (abs((act[\"values\"] - batch[\"values\"])) > self.cliprange)\n",
    "            .to(torch.float)\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        # additional logs: average value predictions, fraction of samples for which we zeroed gradient\n",
    "        self.write(\"additional/value_predictions\", act[\"values\"].mean())\n",
    "        self.write(\"additional/value_loss_zeroed_gradient_fraction\", ratio)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def loss(self, batch):\n",
    "        \"\"\"Computes loss for current batch\"\"\"\n",
    "\n",
    "        # let's run our current policy on this batch\n",
    "        act = self.policy.act(batch[\"observations\"], training=True)\n",
    "\n",
    "        # compute losses\n",
    "        # note that we don't need entropy regularization for this env.\n",
    "        policy_loss = self.policy_loss(batch, act)\n",
    "        critic_loss = self.value_loss(batch, act)\n",
    "\n",
    "        # log all losses\n",
    "        self.write(\"losses\", {\"policy loss\": policy_loss, \"critic loss\": critic_loss})\n",
    "\n",
    "        # Return scalar loss\n",
    "        return policy_loss + self.value_loss_coef * critic_loss\n",
    "\n",
    "    def step(self, batch):\n",
    "        \"\"\"Computes the loss function and performs a single gradient step for this batch.\"\"\"\n",
    "\n",
    "        loss = self.loss(batch)\n",
    "        loss.backward()\n",
    "\n",
    "        clip_grad_norm_(self.policy.model.parameters(), self.max_grad_norm)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        total_norm = 0\n",
    "        for p in self.policy.model.parameters():\n",
    "            param_norm = p.grad.detach().data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm**0.5\n",
    "\n",
    "        # do not forget to clip gradients using self.max_grad_norm\n",
    "        # and log gradient norm\n",
    "        self.write(\"gradient norm\", total_norm)\n",
    "\n",
    "        # this is for logging\n",
    "        self.iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPOModel(17, 6, 64).to(DEVICE)\n",
    "policy = Policy(model)\n",
    "sampler = make_ppo_sampler(env, policy)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, eps=1e-5)\n",
    "ppo = PPO(policy, optimizer, sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "for i in range(epochs):\n",
    "    for minibatch in sampler.get_next():\n",
    "        ppo.step(minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your model just in case\n",
    "\n",
    "\n",
    "def save(model, env, name):\n",
    "    torch.save(model.state_dict(), name)\n",
    "    np.save(name + \"_mean_ob\", env.obs_rmv.mean)\n",
    "    np.save(name + \"_var_ob\", env.obs_rmv.var)\n",
    "    np.save(name + \"_count_ob\", env.obs_rmv.count)\n",
    "\n",
    "\n",
    "save(model, env, \"PPO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model\n",
    "\n",
    "\n",
    "def load(model, env, name):\n",
    "    model.load_state_dict(torch.load(name))\n",
    "    env.obs_rmv.mean = np.load(name + \"_mean_ob.npy\")\n",
    "    env.obs_rmv.var = np.load(name + \"_var_ob.npy\")\n",
    "    env.obs_rmv.count = np.load(name + \"_count_ob.npy\")\n",
    "\n",
    "\n",
    "load(model, env, \"PPO\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, policy, n_games=1, t_max=1000):\n",
    "    \"\"\"\n",
    "    Plays n_games and returns rewards and rendered games\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "\n",
    "        R = 0\n",
    "        for _ in range(t_max):\n",
    "            action = policy.act(np.array([s]))[\"actions\"][0]\n",
    "\n",
    "            s, _, done, info = env.step(action)\n",
    "\n",
    "            # remember that we used a wrapper that normalizes reward\n",
    "            # original reward per step comes here\n",
    "            R += info[\"original reward\"]\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(R)\n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your score: 1508.9040631068149\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "# evaluation will take some time!\n",
    "sessions = evaluate(env, policy, n_games=20)\n",
    "score = sessions.mean()\n",
    "print(f\"Your score: {score}\")\n",
    "\n",
    "assert score >= 1000, \"Needs more training?\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's hope this will work\n",
    "# don't forget to pray\n",
    "env = gym.wrappers.Monitor(env, directory=\"videos\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([391.43007187])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# record sessions\n",
    "# note that t_max is 300, so collected reward will be smaller than 1000\n",
    "evaluate(env, policy, n_games=1, t_max=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
